{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199c6441-b494-407e-b0aa-c2309712de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Script running in: C:\\old_D\\Coding\\Jupyter_all\\Olist_Operations_Project\\02_Scripts\n",
      "ðŸ”Ž Looking for data in: C:\\old_D\\Coding\\Jupyter_all\\Olist_Operations_Project\\01_Raw_Data\n",
      "âœ… Connected to database at: ../03_Database\\olist.db\n",
      "ðŸš€ Loaded orders: 99,441 rows\n",
      "ðŸš€ Loaded order_items: 112,650 rows\n",
      "ðŸš€ Loaded order_payments: 103,886 rows\n",
      "ðŸš€ Loaded order_reviews: 99,224 rows\n",
      "ðŸš€ Loaded products: 32,951 rows\n",
      "ðŸš€ Loaded sellers: 3,095 rows\n",
      "ðŸš€ Loaded customers: 99,441 rows\n",
      "ðŸš€ Loaded geolocation: 1,000,163 rows\n",
      "\n",
      "ðŸ Ingestion Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP PATHS (The Fix) ---\n",
    "# \"../\" means \"Go up one folder level\"\n",
    "raw_data_path = '../01_Raw_Data/'\n",
    "output_folder = '../03_Database' \n",
    "db_path = os.path.join(output_folder, 'olist.db')\n",
    "\n",
    "# Safety Check: Create the folder in the ROOT directory if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"ðŸ“‚ Created missing folder: {output_folder}\")\n",
    "\n",
    "# --- 2. FILE MAPPING ---\n",
    "files = {\n",
    "    'olist_orders_dataset.csv': 'orders',\n",
    "    'olist_order_items_dataset.csv': 'order_items',\n",
    "    'olist_order_payments_dataset.csv': 'order_payments',\n",
    "    'olist_order_reviews_dataset.csv': 'order_reviews',\n",
    "    'olist_products_dataset.csv': 'products',\n",
    "    'olist_sellers_dataset.csv': 'sellers',\n",
    "    'olist_customers_dataset.csv': 'customers',\n",
    "    'olist_geolocation_dataset.csv': 'geolocation'\n",
    "}\n",
    "\n",
    "# --- 3. THE ENGINE ---\n",
    "def ingest_data():\n",
    "    # Print where we are looking (for debugging)\n",
    "    print(f\"ðŸ“ Script running in: {os.getcwd()}\")\n",
    "    print(f\"ðŸ”Ž Looking for data in: {os.path.abspath(raw_data_path)}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    print(f\"âœ… Connected to database at: {db_path}\")\n",
    "\n",
    "    for csv_file, table_name in files.items():\n",
    "        file_path = os.path.join(raw_data_path, csv_file)\n",
    "        \n",
    "        # Check if CSV actually exists before trying to read\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âš ï¸ WARNING: Could not find {csv_file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Basic Cleaning: Lowercase columns, replace spaces with underscores\n",
    "            df.columns = [c.lower().replace(' ', '_') for c in df.columns]\n",
    "            \n",
    "            # Load to DB\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            print(f\"ðŸš€ Loaded {table_name}: {len(df):,} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error on {table_name}: {e}\")\n",
    "            \n",
    "    conn.close()\n",
    "    print(\"\\nðŸ Ingestion Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5c076d-c924-4917-a8be-8e21c512c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Extracted for Lab Analysis.\n",
      "Cohort Size: 7670 rows\n",
      "Regression Size: 110005 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Connect to the Vault\n",
    "conn = sqlite3.connect('../03_Database/olist.db')\n",
    "\n",
    "# 2. Extract the Raw Cohorts (No Averages!)\n",
    "# We pull two specific categories to fight in the \"Arena\"\n",
    "query_cohorts = \"\"\"\n",
    "SELECT \n",
    "    p.product_category_name,\n",
    "    JULIANDAY(o.order_delivered_customer_date) - JULIANDAY(o.order_delivered_carrier_date) as transit_days,\n",
    "    oi.freight_value,\n",
    "    oi.price\n",
    "FROM order_items oi\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE \n",
    "    o.order_status = 'delivered'\n",
    "    AND o.order_delivered_customer_date IS NOT NULL\n",
    "    AND o.order_delivered_carrier_date IS NOT NULL\n",
    "    -- Filter for our two fighters\n",
    "    AND p.product_category_name IN ('cama_mesa_banho', 'beleza_saude')\n",
    "    AND s.seller_state = 'SP' AND c.customer_state = 'SP' -- Control for Geography (SP -> SP only)\n",
    "\"\"\"\n",
    "# Note: The query above needs the joins for sellers/customers to filter SP->SP. \n",
    "# Let's fix the query to include those joins.\n",
    "query_cohorts_fixed = \"\"\"\n",
    "SELECT \n",
    "    p.product_category_name,\n",
    "    JULIANDAY(o.order_delivered_customer_date) - JULIANDAY(o.order_delivered_carrier_date) as transit_days\n",
    "FROM order_items oi\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "WHERE \n",
    "    o.order_status = 'delivered'\n",
    "    AND o.order_delivered_customer_date IS NOT NULL\n",
    "    AND p.product_category_name IN ('cama_mesa_banho', 'beleza_saude')\n",
    "    AND s.seller_state = 'SP' AND c.customer_state = 'SP'\n",
    "\"\"\"\n",
    "\n",
    "df_cohorts = pd.read_sql_query(query_cohorts_fixed, conn)\n",
    "\n",
    "# 3. Extract Data for Regression (Delay vs Stars)\n",
    "query_regression = \"\"\"\n",
    "SELECT \n",
    "    (JULIANDAY(o.order_delivered_customer_date) - JULIANDAY(o.order_estimated_delivery_date)) as delay_days,\n",
    "    r.review_score\n",
    "FROM order_items oi\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN order_reviews r ON oi.order_id = r.order_id\n",
    "WHERE \n",
    "    o.order_status = 'delivered'\n",
    "    AND o.order_delivered_customer_date IS NOT NULL\n",
    "    -- We only care about LATE orders to measure the \"Anger Penalty\"\n",
    "    -- Or we can look at all orders to see the curve\n",
    "\"\"\"\n",
    "df_reg = pd.read_sql_query(query_regression, conn)\n",
    "\n",
    "print(\"âœ… Data Extracted for Lab Analysis.\")\n",
    "print(f\"Cohort Size: {len(df_cohorts)} rows\")\n",
    "print(f\"Regression Size: {len(df_reg)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25948917-5420-4124-a762-a1a6f37c3215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DESCRIPTIVE STATS (SP -> SP) ---\n",
      "Bed/Bath (n=4633): Mean=5.97 days, Std=5.16\n",
      "Beauty   (n=3037):   Mean=4.07 days, Std=4.42\n",
      "\n",
      "--- INFERENTIAL STATS (T-TEST) ---\n",
      "T-Statistic: 17.2785\n",
      "P-Value:     0.00000000000000000000\n",
      "ðŸš¨ RESULT: STATISTICALLY SIGNIFICANT.\n",
      "The difference is REAL. Bed/Bath uses a structurally different process.\n"
     ]
    }
   ],
   "source": [
    "# 1. Isolate the two groups\n",
    "bed_bath = df_cohorts[df_cohorts['product_category_name'] == 'cama_mesa_banho']['transit_days']\n",
    "beauty = df_cohorts[df_cohorts['product_category_name'] == 'beleza_saude']['transit_days']\n",
    "\n",
    "# 2. Descriptive Stats (The \"Eyeball\" Test)\n",
    "print(\"--- DESCRIPTIVE STATS (SP -> SP) ---\")\n",
    "print(f\"Bed/Bath (n={len(bed_bath)}): Mean={bed_bath.mean():.2f} days, Std={bed_bath.std():.2f}\")\n",
    "print(f\"Beauty   (n={len(beauty)}):   Mean={beauty.mean():.2f} days, Std={beauty.std():.2f}\")\n",
    "\n",
    "# 3. The T-Test (The \"Gavel\")\n",
    "# equal_var=False because we assume they might have different variances (Welch's t-test)\n",
    "t_stat, p_val = stats.ttest_ind(bed_bath, beauty, equal_var=False)\n",
    "\n",
    "print(\"\\n--- INFERENTIAL STATS (T-TEST) ---\")\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value:     {p_val:.20f}\") # High precision to see if it's 0.000...\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"ðŸš¨ RESULT: STATISTICALLY SIGNIFICANT.\")\n",
    "    print(\"The difference is REAL. Bed/Bath uses a structurally different process.\")\n",
    "else:\n",
    "    print(\"RESULT: Not Significant. The difference is just luck/noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8ef1da-a1d1-46cb-ac8e-4a1e900f1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           review_score   R-squared:                       0.285\n",
      "Model:                            OLS   Adj. R-squared:                  0.285\n",
      "Method:                 Least Squares   F-statistic:                     7248.\n",
      "Date:                Thu, 18 Dec 2025   Prob (F-statistic):               0.00\n",
      "Time:                        17:31:54   Log-Likelihood:                -31752.\n",
      "No. Observations:               18151   AIC:                         6.351e+04\n",
      "Df Residuals:                   18149   BIC:                         6.352e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.6500      0.011    340.290      0.000       3.629       3.671\n",
      "delay_days    -0.1303      0.002    -85.136      0.000      -0.133      -0.127\n",
      "==============================================================================\n",
      "Omnibus:                      882.116   Durbin-Watson:                   1.713\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              837.345\n",
      "Skew:                          -0.475   Prob(JB):                    1.49e-182\n",
      "Kurtosis:                       2.548   Cond. No.                         7.29\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# 1. Clean Data for Regression\n",
    "# Remove nulls\n",
    "df_reg_clean = df_reg.dropna()\n",
    "\n",
    "# 2. Filter for \"Late\" orders only (Delay > 0) to see the punishment curve\n",
    "# Or keep all to see the full picture. Let's look at the range -5 to +20 days\n",
    "df_focus = df_reg_clean[ (df_reg_clean['delay_days'] > -5) & (df_reg_clean['delay_days'] < 30) ]\n",
    "\n",
    "# 3. Run Simple Linear Regression (Stars ~ Delay)\n",
    "# Y = Review Score, X = Delay Days\n",
    "X = sm.add_constant(df_focus['delay_days']) # Adds the intercept\n",
    "model = sm.OLS(df_focus['review_score'], X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffd0863-f905-4081-9e59-f4818c9a4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REQUEST A: SLOW VS LATE ===\n",
      "Total Late Orders: 8.1%\n",
      "\n",
      "--- Actual Duration Stats (Late vs On-Time) ---\n",
      "         mean   std  count\n",
      "is_late                   \n",
      "False    10.9   6.2  88644\n",
      "True     31.5  17.3   7826\n",
      "\n",
      "\n",
      "=== REQUEST B: THE STEP FUNCTION (Days) ===\n",
      "               mean  median  std\n",
      "step_approval   0.4     0.0  0.9\n",
      "step_handling   2.8     1.8  3.5\n",
      "step_transit    9.3     7.1  8.8\n",
      "\n",
      "\n",
      "=== REQUEST C: THE PARETO CHECK (Top 5 States) ===\n",
      "                Volume  Avg Transit Days\n",
      "customer_state                          \n",
      "SP               40493               5.6\n",
      "RJ               12350              12.0\n",
      "MG               11354               8.8\n",
      "RS                5344              12.1\n",
      "PR                4923               8.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONNECT & EXTRACT ---\n",
    "# We fetch the raw log needed to reconstruct the timeline\n",
    "db_path = '../03_Database/olist.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.order_status,\n",
    "    o.order_purchase_timestamp,\n",
    "    o.order_approved_at,\n",
    "    o.order_delivered_carrier_date,\n",
    "    o.order_delivered_customer_date,\n",
    "    o.order_estimated_delivery_date,\n",
    "    c.customer_state\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# --- 2. THE REFINERY (Time Conversions) ---\n",
    "# Convert strings to datetime objects for math\n",
    "time_cols = ['order_purchase_timestamp', 'order_approved_at', \n",
    "             'order_delivered_carrier_date', 'order_delivered_customer_date', \n",
    "             'order_estimated_delivery_date']\n",
    "\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# --- REQUEST A: \"SLOW VS LATE\" (Physics vs Psychology) ---\n",
    "# Actual Duration (Physics)\n",
    "df['actual_duration'] = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400\n",
    "\n",
    "# Promise Gap (Psychology): Estimate - Actual\n",
    "# Negative = Late (Delivered AFTER Estimate)\n",
    "df['promise_gap'] = (df['order_estimated_delivery_date'] - df['order_delivered_customer_date']).dt.total_seconds() / 86400\n",
    "df['is_late'] = df['promise_gap'] < 0\n",
    "\n",
    "# --- REQUEST B: THE \"STEP FUNCTION\" (Bottleneck Detection) ---\n",
    "# 1. Approval (Purchase -> Approved)\n",
    "df['step_approval'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400\n",
    "# 2. Handling (Approved -> Carrier)\n",
    "df['step_handling'] = (df['order_delivered_carrier_date'] - df['order_approved_at']).dt.total_seconds() / 86400\n",
    "# 3. Transit (Carrier -> Customer)\n",
    "df['step_transit'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# --- OUTPUT GENERATION ---\n",
    "\n",
    "print(\"\\n=== REQUEST A: SLOW VS LATE ===\")\n",
    "late_pct = (df['is_late'].sum() / len(df)) * 100\n",
    "print(f\"Total Late Orders: {late_pct:.1f}%\")\n",
    "\n",
    "print(\"\\n--- Actual Duration Stats (Late vs On-Time) ---\")\n",
    "# We group by 'is_late' to see if late orders are physically slower or just had bad estimates\n",
    "print(df.groupby('is_late')['actual_duration'].agg(['mean', 'std', 'count']).round(1))\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== REQUEST B: THE STEP FUNCTION (Days) ===\")\n",
    "# We verify where the time is actually spent\n",
    "steps = ['step_approval', 'step_handling', 'step_transit']\n",
    "stats_table = df[steps].agg(['mean', 'median', 'std']).T.round(1)\n",
    "print(stats_table)\n",
    "\n",
    "\n",
    "print(\"\\n\\n=== REQUEST C: THE PARETO CHECK (Top 5 States) ===\")\n",
    "# Where is the pain concentrated?\n",
    "pareto = df.groupby('customer_state')['step_transit'].agg(['count', 'mean'])\n",
    "pareto = pareto.sort_values('count', ascending=False).head(5).round(1)\n",
    "pareto.columns = ['Volume', 'Avg Transit Days']\n",
    "print(pareto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392be1e6-c439-4fe0-8e58-acd8c0a5134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REQUEST A: TOP 10 ROUTES (Geography) ===\n",
      "          Volume  Avg Transit  Std Dev  Late %\n",
      "route                                         \n",
      "SP -> SP   35420          4.7      5.1     6.1\n",
      "SP -> RJ    9403         12.6     10.7    14.8\n",
      "SP -> MG    8567          9.0      5.9     6.1\n",
      "SP -> RS    4133         12.7      9.2     7.2\n",
      "SP -> PR    3609          9.4      6.2     5.2\n",
      "PR -> SP    3339          7.6      5.9     3.7\n",
      "MG -> SP    2900          7.7      5.9     4.1\n",
      "SP -> SC    2709         12.4      7.7    10.6\n",
      "SP -> BA    2626         16.3     10.0    14.8\n",
      "MG -> MG    1669          5.5      5.2     3.1\n",
      "\n",
      "[Statistical Test] SP->SP vs SP->RJ:\n",
      "   Avg Difference: 7.9 days slower\n",
      "   P-Value: 0.00000 (SIGNIFICANT)\n",
      "\n",
      "\n",
      "=== REQUEST B: TOP 5 CATEGORIES (Physics) ===\n",
      "                        Volume  Avg Transit  Std Dev  Late %\n",
      "product_category_name                                       \n",
      "cama_mesa_banho          10953          9.6      8.2     8.4\n",
      "beleza_saude              9465          8.9      8.6     9.1\n",
      "esporte_lazer             8430          9.4      8.6     7.4\n",
      "moveis_decoracao          8159          9.1      8.8     8.4\n",
      "informatica_acessorios    7643          9.6      8.4     7.8\n",
      "\n",
      "[Statistical Test] Bed_Bath vs Health_Beauty:\n",
      "   Avg Difference: 0.8 days slower\n",
      "   P-Value: 0.00000 (SIGNIFICANT)\n",
      "\n",
      "\n",
      "=== REQUEST C: THE RIO BLACK HOLE (Destination = RJ) ===\n",
      "              Volume  Avg Transit  Std Dev  Late %\n",
      "seller_state                                      \n",
      "SP              9403         12.6     10.7    14.8\n",
      "MG              1275         10.9      9.4     9.1\n",
      "PR              1115         13.5     10.9    13.1\n",
      "RJ              1084          3.7      6.0     6.2\n",
      "SC               531         12.9     10.2    10.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# --- 1. CONNECT & EXTRACT (The \"Wide Table\" Join) ---\n",
    "db_path = '../03_Database/olist.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# We need a more complex join now to get Product Categories and Seller Locations\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.order_status,\n",
    "    o.order_delivered_carrier_date,\n",
    "    o.order_delivered_customer_date,\n",
    "    o.order_estimated_delivery_date,\n",
    "    c.customer_state,\n",
    "    s.seller_state,\n",
    "    p.product_category_name\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "-- We assume 1 item per order for the dominant logic (simplification for route analysis)\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# --- 2. PREPROCESSING ---\n",
    "# Convert dates\n",
    "cols = ['order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "for col in cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Calculate Core Metrics\n",
    "# Transit Time (Carrier -> Customer)\n",
    "df['transit_days'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# Late Flag (Actual > Estimate)\n",
    "df['is_late'] = df['order_delivered_customer_date'] > df['order_estimated_delivery_date']\n",
    "\n",
    "# Create Route Column\n",
    "df['route'] = df['seller_state'] + ' -> ' + df['customer_state']\n",
    "\n",
    "# --- HELPER FUNCTION FOR CONSISTENT REPORTING ---\n",
    "def get_summary(grouped_df):\n",
    "    summary = grouped_df.agg({\n",
    "        'transit_days': ['count', 'mean', 'std'],\n",
    "        'is_late': 'mean' # Mean of boolean (0/1) gives percentage\n",
    "    })\n",
    "    # Flatten columns\n",
    "    summary.columns = ['Volume', 'Avg Transit', 'Std Dev', 'Late %']\n",
    "    summary['Late %'] = (summary['Late %'] * 100).round(1)\n",
    "    summary[['Avg Transit', 'Std Dev']] = summary[['Avg Transit', 'Std Dev']].round(1)\n",
    "    return summary\n",
    "\n",
    "# --- REQUEST A: THE ROUTE MATRIX (Geography) ---\n",
    "print(\"=== REQUEST A: TOP 10 ROUTES (Geography) ===\")\n",
    "route_stats = get_summary(df.groupby('route'))\n",
    "top_routes = route_stats.sort_values('Volume', ascending=False).head(10)\n",
    "print(top_routes)\n",
    "\n",
    "# T-TEST: SP->SP vs SP->RJ\n",
    "sp_sp = df[df['route'] == 'SP -> SP']['transit_days'].dropna()\n",
    "sp_rj = df[df['route'] == 'SP -> RJ']['transit_days'].dropna()\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(sp_sp, sp_rj, equal_var=False)\n",
    "print(f\"\\n[Statistical Test] SP->SP vs SP->RJ:\")\n",
    "print(f\"   Avg Difference: {sp_rj.mean() - sp_sp.mean():.1f} days slower\")\n",
    "print(f\"   P-Value: {p_val:.5f} ({'SIGNIFICANT' if p_val < 0.05 else 'NOISE'})\")\n",
    "\n",
    "\n",
    "# --- REQUEST B: THE PHYSICS CHECK (Product) ---\n",
    "print(\"\\n\\n=== REQUEST B: TOP 5 CATEGORIES (Physics) ===\")\n",
    "cat_stats = get_summary(df.groupby('product_category_name'))\n",
    "top_cats = cat_stats.sort_values('Volume', ascending=False).head(5)\n",
    "print(top_cats)\n",
    "\n",
    "# T-TEST: Bed_Bath vs Health_Beauty\n",
    "bed_bath = df[df['product_category_name'] == 'cama_mesa_banho']['transit_days'].dropna()\n",
    "health = df[df['product_category_name'] == 'beleza_saude']['transit_days'].dropna()\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(bed_bath, health, equal_var=False)\n",
    "print(f\"\\n[Statistical Test] Bed_Bath vs Health_Beauty:\")\n",
    "print(f\"   Avg Difference: {bed_bath.mean() - health.mean():.1f} days slower\")\n",
    "print(f\"   P-Value: {p_val:.5f} ({'SIGNIFICANT' if p_val < 0.05 else 'NOISE'})\")\n",
    "\n",
    "\n",
    "# --- REQUEST C: THE RIO BLACK HOLE (Drill-Down) ---\n",
    "print(\"\\n\\n=== REQUEST C: THE RIO BLACK HOLE (Destination = RJ) ===\")\n",
    "rj_dest = df[df['customer_state'] == 'RJ']\n",
    "rj_stats = get_summary(rj_dest.groupby('seller_state'))\n",
    "# Show top 5 origins sending TO Rio\n",
    "print(rj_stats.sort_values('Volume', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a05684-32f1-4885-8551-b54dec92972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REQUEST B: THE PROMISE AUDIT ===\n",
      "          Avg Promise (Days)  Std Promise  Avg Actual (Days)  Std Actual  \\\n",
      "route                                                                      \n",
      "SP -> RJ                23.0          8.0               12.6        10.7   \n",
      "SP -> RS                26.5          7.0               12.7         9.2   \n",
      "\n",
      "          Avg Safety Buffer  Late %  \n",
      "route                                \n",
      "SP -> RJ               10.4    10.0  \n",
      "SP -> RS               13.8    10.0  \n",
      "\n",
      "\n",
      "=== REQUEST C: THE FREIGHT VALUE CHECK (SP -> RJ) ===\n",
      "                Min Price  Max Price  Avg Price  Avg Transit  Std Transit  \\\n",
      "freight_bucket                                                              \n",
      "1. Low Cost           0.0       15.2       13.6         11.9         10.1   \n",
      "2. Med-Low            NaN        NaN        NaN          NaN          NaN   \n",
      "3. Med-High           NaN        NaN        NaN          NaN          NaN   \n",
      "4. High Cost         19.8      200.5       33.9         13.3         11.5   \n",
      "\n",
      "                Volume  \n",
      "freight_bucket          \n",
      "1. Low Cost       2593  \n",
      "2. Med-Low           0  \n",
      "3. Med-High          0  \n",
      "4. High Cost      2346  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3020\\2355539657.py:94: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  stats_c = df_rj_filtered.groupby('freight_bucket').agg({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONNECT & EXTRACT ---\n",
    "db_path = '../03_Database/olist.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# We need Estimated Date, Delivered Date, Freight Value, and Geography\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.order_estimated_delivery_date,\n",
    "    o.order_delivered_customer_date,\n",
    "    o.order_delivered_carrier_date,\n",
    "    oi.freight_value,\n",
    "    s.seller_state,\n",
    "    c.customer_state\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# --- 2. THE REFINERY ---\n",
    "# Date Conversions\n",
    "cols = ['order_estimated_delivery_date', 'order_delivered_customer_date', 'order_delivered_carrier_date']\n",
    "for col in cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Feature Engineering\n",
    "df['route'] = df['seller_state'] + ' -> ' + df['customer_state']\n",
    "\n",
    "# Metric: The Promise (Estimated Days from Carrier Handoff to Customer)\n",
    "# Note: Usually Promise is calculated from Purchase, but for Logistics ops we often look at Transit Promise\n",
    "# Let's stick to the User's \"Transit\" focus: Delivered - Carrier vs Estimated - Carrier\n",
    "# actually, \"Estimated Delivery\" is usually usually from Purchase. Let's assume standard \"Purchase to Delivery\" for simplicity of \"Promise\"\n",
    "# But the prompt asks for \"Transit Time\" context. Let's use (Estimated - Delivered) as the \"Buffer\".\n",
    "\n",
    "# Actual Transit (Carrier -> Customer)\n",
    "df['actual_transit'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# The \"buffer\" or \"error\" in the promise: (Estimated - Actual)\n",
    "# Positive = Customer Happy (Arrived before estimate)\n",
    "# Negative = Customer Angry (Arrived after estimate)\n",
    "df['promise_gap'] = (df['order_estimated_delivery_date'] - df['order_delivered_customer_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# The \"Promised Duration\" (Implied)\n",
    "# We don't have Purchase date in this specific query, but we have the gap. \n",
    "# Let's infer: Promised_Transit = Actual_Transit + Promise_Gap\n",
    "df['promised_transit'] = df['actual_transit'] + df['promise_gap']\n",
    "# Note: This is an approximation because 'Estimated' is fixed, 'Actual' varies. \n",
    "# Ideally: Promised_Date - Carrier_Date = Promised_Transit. Let's use that.\n",
    "df['promised_transit_days'] = (df['order_estimated_delivery_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "\n",
    "\n",
    "# --- REQUEST B: THE PROMISE AUDIT (SP->RJ vs SP->RS) ---\n",
    "print(\"=== REQUEST B: THE PROMISE AUDIT ===\")\n",
    "target_routes = ['SP -> RJ', 'SP -> RS']\n",
    "df_b = df[df['route'].isin(target_routes)].copy()\n",
    "\n",
    "# We calculate the \"Late Rate\" for context\n",
    "df_b['is_late'] = df_b['promise_gap'] < 0\n",
    "\n",
    "# Aggregation\n",
    "stats_b = df_b.groupby('route').agg({\n",
    "    'promised_transit_days': ['mean', 'std'],\n",
    "    'actual_transit': ['mean', 'std'],\n",
    "    'promise_gap': ['mean'],\n",
    "    'is_late': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "# Clean up for display\n",
    "stats_b.columns = ['Avg Promise (Days)', 'Std Promise', 'Avg Actual (Days)', 'Std Actual', 'Avg Safety Buffer', 'Late %']\n",
    "stats_b['Late %'] = (stats_b['Late %'] * 100).round(1)\n",
    "print(stats_b)\n",
    "\n",
    "\n",
    "# --- REQUEST C: THE FREIGHT VALUE CHECK (SP->RJ Only) ---\n",
    "print(\"\\n\\n=== REQUEST C: THE FREIGHT VALUE CHECK (SP -> RJ) ===\")\n",
    "df_rj = df[df['route'] == 'SP -> RJ'].copy()\n",
    "\n",
    "# Create Quartiles for Freight Value\n",
    "# q=4 means Quartiles: 0-25% (Low), 25-50%, 50-75%, 75-100% (High)\n",
    "df_rj['freight_bucket'] = pd.qcut(df_rj['freight_value'], q=4, labels=['1. Low Cost', '2. Med-Low', '3. Med-High', '4. High Cost'])\n",
    "\n",
    "# Filter for just Low (Bottom 25%) and High (Top 25%) as requested\n",
    "df_rj_filtered = df_rj[df_rj['freight_bucket'].isin(['1. Low Cost', '4. High Cost'])]\n",
    "\n",
    "stats_c = df_rj_filtered.groupby('freight_bucket').agg({\n",
    "    'freight_value': ['min', 'max', 'mean'],\n",
    "    'actual_transit': ['mean', 'std', 'count']\n",
    "}).round(1)\n",
    "\n",
    "stats_c.columns = ['Min Price', 'Max Price', 'Avg Price', 'Avg Transit', 'Std Transit', 'Volume']\n",
    "print(stats_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009a1726-a4d4-4edd-bb05-7b50bf0553d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REQUEST A: BUFFER SIMULATION (SP -> RJ) ===\n",
      "             Metric  Current State (Baseline)  Simulated State (+4 Days)\n",
      "Late Orders (Count)                    1396.0                     1031.0\n",
      "      Late Rate (%)                      14.8                       11.0\n",
      "\n",
      " IMPACT: We saved 365 customers from disappointment.\n",
      "\n",
      "\n",
      "=== REQUEST B: HIGH VALUE BUFFER CHECK ===\n",
      "Segment: High Freight Cost (> $19.75)\n",
      "Avg Actual Transit:   13.2 days\n",
      "Avg Promised Transit: 23.6 days\n",
      "--------------------------------\n",
      "Current Safety Buffer: 10.3 days\n",
      "Transit Variability (Std): 11.5 days\n",
      "\n",
      "[VERDICT]\n",
      "FAIL. Buffer (10.3) is smaller than Variability (11.5).\n",
      "We are setting ourselves up to fail whenever a minor delay happens.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONNECT & EXTRACT ---\n",
    "db_path = '../03_Database/olist.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# We extract SP->RJ route data specifically for the simulation\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.order_estimated_delivery_date,\n",
    "    o.order_delivered_customer_date,\n",
    "    o.order_delivered_carrier_date,\n",
    "    oi.freight_value,\n",
    "    s.seller_state,\n",
    "    c.customer_state\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "JOIN order_items oi ON o.order_id = oi.order_id\n",
    "JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "WHERE \n",
    "    o.order_status = 'delivered'\n",
    "    AND s.seller_state = 'SP' \n",
    "    AND c.customer_state = 'RJ'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# --- 2. PREPROCESSING ---\n",
    "cols = ['order_estimated_delivery_date', 'order_delivered_customer_date', 'order_delivered_carrier_date']\n",
    "for col in cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Current State Metrics\n",
    "df['actual_transit'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "# Current Late Flag\n",
    "df['is_late_current'] = df['order_delivered_customer_date'] > df['order_estimated_delivery_date']\n",
    "\n",
    "\n",
    "# --- REQUEST A: THE BUFFER SIMULATION (+4 Days) ---\n",
    "print(\"=== REQUEST A: BUFFER SIMULATION (SP -> RJ) ===\")\n",
    "\n",
    "# Create the Simulation: What if we promised 4 days MORE?\n",
    "# We add 4 days (4 * 24 * 3600 seconds) to the estimate\n",
    "df['simulated_estimate_date'] = df['order_estimated_delivery_date'] + pd.Timedelta(days=4)\n",
    "\n",
    "# Calculate Simulated Late Flag\n",
    "df['is_late_simulated'] = df['order_delivered_customer_date'] > df['simulated_estimate_date']\n",
    "\n",
    "# The Comparison Table\n",
    "sim_results = pd.DataFrame({\n",
    "    'Metric': ['Late Orders (Count)', 'Late Rate (%)'],\n",
    "    'Current State (Baseline)': [\n",
    "        df['is_late_current'].sum(), \n",
    "        (df['is_late_current'].mean() * 100).round(1)\n",
    "    ],\n",
    "    'Simulated State (+4 Days)': [\n",
    "        df['is_late_simulated'].sum(), \n",
    "        (df['is_late_simulated'].mean() * 100).round(1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(sim_results.to_string(index=False))\n",
    "print(f\"\\n IMPACT: We saved {df['is_late_current'].sum() - df['is_late_simulated'].sum()} customers from disappointment.\")\n",
    "\n",
    "\n",
    "# --- REQUEST B: THE HONESTY CHECK (High Value) ---\n",
    "print(\"\\n\\n=== REQUEST B: HIGH VALUE BUFFER CHECK ===\")\n",
    "\n",
    "# Filter for High Cost (Top 25% of freight value)\n",
    "high_cost_thresh = df['freight_value'].quantile(0.75)\n",
    "df_high = df[df['freight_value'] >= high_cost_thresh].copy()\n",
    "\n",
    "# Calculate the \"Promise\" (Estimate - Carrier Date) vs \"Actual\" (Customer - Carrier Date)\n",
    "# This tells us how many days we *allocated* for the trip vs how many it *took*\n",
    "df_high['promised_transit'] = (df_high['order_estimated_delivery_date'] - df_high['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# Calculate the Safety Buffer (Promise - Actual)\n",
    "df_high['safety_buffer'] = df_high['promised_transit'] - df_high['actual_transit']\n",
    "\n",
    "# Stats\n",
    "avg_actual = df_high['actual_transit'].mean()\n",
    "avg_promise = df_high['promised_transit'].mean()\n",
    "avg_buffer = df_high['safety_buffer'].mean()\n",
    "std_actual = df_high['actual_transit'].std()\n",
    "\n",
    "print(f\"Segment: High Freight Cost (> ${high_cost_thresh:.2f})\")\n",
    "print(f\"Avg Actual Transit:   {avg_actual:.1f} days\")\n",
    "print(f\"Avg Promised Transit: {avg_promise:.1f} days\")\n",
    "print(f\"--------------------------------\")\n",
    "print(f\"Current Safety Buffer: {avg_buffer:.1f} days\")\n",
    "print(f\"Transit Variability (Std): {std_actual:.1f} days\")\n",
    "\n",
    "# The Verdict logic\n",
    "print(f\"\\n[VERDICT]\")\n",
    "if avg_buffer < std_actual:\n",
    "    print(f\"FAIL. Buffer ({avg_buffer:.1f}) is smaller than Variability ({std_actual:.1f}).\")\n",
    "    print(\"We are setting ourselves up to fail whenever a minor delay happens.\")\n",
    "else:\n",
    "    print(f\"PASS. Buffer ({avg_buffer:.1f}) covers the Variability ({std_actual:.1f}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38029a4-b8d4-4c75-89f7-c6d2d6f37be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ï¸ Building the Master Analytical Table (ABT)...\n",
      "âœ… Master Table Saved: 110197 rows. (Use this CSV from now on!)\n",
      "\n",
      "=== REQUEST A: FAILURE SPECTRUM (By Severity) ===\n",
      "lateness_tier  1. On Time / Early  2. Minor Late (1-3 Days)  \\\n",
      "route                                                         \n",
      "SP -> SP                     93.9                       2.9   \n",
      "SP -> RJ                     85.2                       3.2   \n",
      "SP -> MG                     93.9                       2.4   \n",
      "SP -> RS                     92.7                       2.0   \n",
      "SP -> PR                     94.8                       2.1   \n",
      "\n",
      "lateness_tier  3. Major Late (>3 Days)  Total Volume  \n",
      "route                                                 \n",
      "SP -> SP                           3.2         35425  \n",
      "SP -> RJ                          11.7          9403  \n",
      "SP -> MG                           3.7          8567  \n",
      "SP -> RS                           5.2          4134  \n",
      "SP -> PR                           3.1          3609  \n",
      "\n",
      "\n",
      "=== REQUEST B: RED ZONE AUTOPSY (Who caused the catastrophe?) ===\n",
      "Total Major Failures: 5731\n",
      "  Seller State  Red Zone Volume  % of All Crises\n",
      "0           SP             4377             76.4\n",
      "1           PR              366              6.4\n",
      "2           MG              341              6.0\n",
      "3           RJ              257              4.5\n",
      "4           SC              153              2.7\n",
      "\n",
      "[SP -> RJ] Contribution to the Crisis:\n",
      "Volume: 1099 orders\n",
      "Share:  19.2% of all Major Delays come from this one route.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- PART 1: THE ARCHITECT (Create the Master File) ---\n",
    "db_path = '../03_Database/olist.db' # Ensure this matches your folder\n",
    "csv_path = 'master_logistics_table.csv'\n",
    "\n",
    "# Check if we need to regenerate the Master File\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"ðŸ—ï¸ Building the Master Analytical Table (ABT)...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # \"The Great Unification\" Query\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.order_status,\n",
    "        o.order_purchase_timestamp,\n",
    "        o.order_delivered_carrier_date,\n",
    "        o.order_delivered_customer_date,\n",
    "        o.order_estimated_delivery_date,\n",
    "        c.customer_state,\n",
    "        s.seller_state,\n",
    "        p.product_category_name,\n",
    "        oi.freight_value\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Save for future speed (The \"Cache\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… Master Table Saved: {len(df)} rows. (Use this CSV from now on!)\")\n",
    "else:\n",
    "    print(\"âš¡ Loading Master Table from Cache (CSV)...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# --- PART 2: THE ANALYST (Severity Audit) ---\n",
    "\n",
    "# Preprocessing\n",
    "time_cols = ['order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Feature Engineering\n",
    "df['route'] = df['seller_state'] + ' -> ' + df['customer_state']\n",
    "\n",
    "# Lateness Logic: (Actual - Estimated)\n",
    "# Positive = Late, Negative = Early\n",
    "df['lateness_days'] = (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# The Severity Buckets\n",
    "def classify_severity(days):\n",
    "    if days <= 0:\n",
    "        return '1. On Time / Early'\n",
    "    elif days <= 3:\n",
    "        return '2. Minor Late (1-3 Days)'\n",
    "    else:\n",
    "        return '3. Major Late (>3 Days)' # The \"Red Zone\"\n",
    "\n",
    "df['lateness_tier'] = df['lateness_days'].apply(classify_severity)\n",
    "\n",
    "# --- REQUEST A: THE FAILURE SPECTRUM (Top 5 Routes) ---\n",
    "print(\"\\n=== REQUEST A: FAILURE SPECTRUM (By Severity) ===\")\n",
    "# Filter Top 5 Routes\n",
    "top_routes = df['route'].value_counts().head(5).index.tolist()\n",
    "df_top = df[df['route'].isin(top_routes)]\n",
    "\n",
    "# Create the Matrix\n",
    "spectrum = pd.crosstab(\n",
    "    index=df_top['route'],\n",
    "    columns=df_top['lateness_tier'],\n",
    "    normalize='index' # Show as %\n",
    ") * 100\n",
    "\n",
    "# Formatting\n",
    "cols_order = ['1. On Time / Early', '2. Minor Late (1-3 Days)', '3. Major Late (>3 Days)']\n",
    "spectrum = spectrum[cols_order].round(1)\n",
    "spectrum['Total Volume'] = df_top['route'].value_counts()\n",
    "print(spectrum.sort_values('Total Volume', ascending=False))\n",
    "\n",
    "\n",
    "# --- REQUEST B: THE RED ZONE AUTOPSY ---\n",
    "print(\"\\n\\n=== REQUEST B: RED ZONE AUTOPSY (Who caused the catastrophe?) ===\")\n",
    "# Filter ONLY for the Red Zone\n",
    "df_red = df[df['lateness_tier'] == '3. Major Late (>3 Days)']\n",
    "\n",
    "# Who is the worst offender? (Seller State)\n",
    "autopsy = df_red['seller_state'].value_counts().reset_index()\n",
    "autopsy.columns = ['Seller State', 'Red Zone Volume']\n",
    "autopsy['% of All Crises'] = (autopsy['Red Zone Volume'] / len(df_red) * 100).round(1)\n",
    "\n",
    "print(f\"Total Major Failures: {len(df_red)}\")\n",
    "print(autopsy.head(5))\n",
    "\n",
    "# The SP -> RJ Specific Check\n",
    "sp_rj_red = len(df_red[df_red['route'] == 'SP -> RJ'])\n",
    "print(f\"\\n[SP -> RJ] Contribution to the Crisis:\")\n",
    "print(f\"Volume: {sp_rj_red} orders\")\n",
    "print(f\"Share:  {sp_rj_red / len(df_red) * 100:.1f}% of all Major Delays come from this one route.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f9dc7a-da74-4aef-b5a1-1583705c2786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Loading Master Table from Cache...\n",
      "Analyzing 9403 orders on the SP -> RJ Route...\n",
      "\n",
      "=== REQUEST A: HOW BAD IS 'MAJOR LATE'? ===\n",
      "Context: 1099 Major Failures detected.\n",
      "Median Delay: 11.7 days late\n",
      "95th Percentile (The Horror Story): 37.9 days late\n",
      "VERDICT: CATASTROPHIC. We cannot just 'pad' the estimate. The delay is too long.\n",
      "\n",
      "\n",
      "=== REQUEST B: PRODUCT x ROUTE INTERACTION ===\n",
      "                Category  Volume  Major Late %\n",
      "1          esporte_lazer     577          14.0\n",
      "0        cama_mesa_banho    1402          13.6\n",
      "3     relogios_presentes     679          12.2\n",
      "2       moveis_decoracao     781          10.5\n",
      "4  utilidades_domesticas     565           6.7\n",
      "Could not compare specific categories (maybe they aren't in Top 5). Check the table above.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. LOAD THE MASTER FILE ---\n",
    "csv_path = 'master_logistics_table.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"âš¡ Loading Master Table from Cache...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    print(\"âš ï¸ CSV Cache missing. Please run the previous 'Great Unification' script first.\")\n",
    "    # (In a real scenario, we'd trigger a rebuild here, but assuming you have it from the last step)\n",
    "\n",
    "# --- 2. PREPROCESSING & FILTERS ---\n",
    "# Date Conversion\n",
    "time_cols = ['order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "for col in time_cols:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Logic: Route & Lateness\n",
    "df['route'] = df['seller_state'] + ' -> ' + df['customer_state']\n",
    "df['lateness_days'] = (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.total_seconds() / 86400\n",
    "\n",
    "# \"Red Zone\" Definition (> 3 Days Late)\n",
    "df['is_major_late'] = df['lateness_days'] > 3\n",
    "\n",
    "# FILTER: Isolate the \"Crime Scene\" (SP -> RJ)\n",
    "df_rio = df[df['route'] == 'SP -> RJ'].copy()\n",
    "print(f\"Analyzing {len(df_rio)} orders on the SP -> RJ Route...\")\n",
    "\n",
    "\n",
    "# --- REQUEST A: THE \"TAIL OF THE DRAGON\" (Severity Check) ---\n",
    "print(\"\\n=== REQUEST A: HOW BAD IS 'MAJOR LATE'? ===\")\n",
    "# Filter only for the failures\n",
    "rio_failures = df_rio[df_rio['is_major_late'] == True]\n",
    "\n",
    "median_delay = rio_failures['lateness_days'].median()\n",
    "horror_delay = rio_failures['lateness_days'].quantile(0.95)\n",
    "\n",
    "print(f\"Context: {len(rio_failures)} Major Failures detected.\")\n",
    "print(f\"Median Delay: {median_delay:.1f} days late\")\n",
    "print(f\"95th Percentile (The Horror Story): {horror_delay:.1f} days late\")\n",
    "\n",
    "# Verdict Logic\n",
    "if median_delay > 5:\n",
    "    print(\"VERDICT: CATASTROPHIC. We cannot just 'pad' the estimate. The delay is too long.\")\n",
    "else:\n",
    "    print(\"VERDICT: MANAGEABLE. A small buffer increase could fix most of this.\")\n",
    "\n",
    "\n",
    "# --- REQUEST B: THE \"INTERSECTION\" TEST (Product x Route) ---\n",
    "print(\"\\n\\n=== REQUEST B: PRODUCT x ROUTE INTERACTION ===\")\n",
    "# We want to compare the failure rate of Heavy vs Light items WITHIN Rio\n",
    "\n",
    "# 1. Identify Top 5 Categories in Rio by Volume\n",
    "top_cats_rio = df_rio['product_category_name'].value_counts().head(5).index.tolist()\n",
    "df_rio_top = df_rio[df_rio['product_category_name'].isin(top_cats_rio)]\n",
    "\n",
    "# 2. Calculate Major Late % per Category\n",
    "intersection = df_rio_top.groupby('product_category_name').agg({\n",
    "    'lateness_days': 'count',       # Volume\n",
    "    'is_major_late': 'mean'         # Failure Rate\n",
    "}).reset_index()\n",
    "\n",
    "intersection.columns = ['Category', 'Volume', 'Major Late %']\n",
    "intersection['Major Late %'] = (intersection['Major Late %'] * 100).round(1)\n",
    "\n",
    "print(intersection.sort_values('Major Late %', ascending=False))\n",
    "\n",
    "# --- THE FINAL CHECK (Bed/Bath vs Beauty) ---\n",
    "# Is there a massive gap?\n",
    "try:\n",
    "    bb_fail = intersection[intersection['Category'] == 'cama_mesa_banho']['Major Late %'].values[0]\n",
    "    hb_fail = intersection[intersection['Category'] == 'beleza_saude']['Major Late %'].values[0]\n",
    "    \n",
    "    print(f\"\\n[The Comparison]\")\n",
    "    print(f\"Bed/Bath Failure Rate: {bb_fail}%\")\n",
    "    print(f\"Beauty Failure Rate:   {hb_fail}%\")\n",
    "    \n",
    "    gap = bb_fail - hb_fail\n",
    "    if gap > 5:\n",
    "        print(f\"CONCLUSION: SURGICAL STRIKE. Bed/Bath is {gap:.1f}% riskier. Embargo it.\")\n",
    "    else:\n",
    "        print(f\"CONCLUSION: BLANKET FIX. The risk is systemic ({gap:.1f}% diff). Fix the whole route.\")\n",
    "except:\n",
    "    print(\"Could not compare specific categories (maybe they aren't in Top 5). Check the table above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c843d2b-a5f9-44a4-8c36-20c6d6fcd097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Loading Master Table from Cache...\n",
      "\n",
      "=== HYPOTHESIS A: RIO CHAOS (Variance Test) ===\n",
      "Sample Sizes: Rio (n=9403), RS (n=4133)\n",
      "Rio Std Dev: 10.69 days\n",
      "RS Std Dev:  9.17 days\n",
      "Levene Statistic: 46.1980\n",
      "P-Value: 0.00000000001113606700\n",
      "\n",
      "[VERDICT]\n",
      "ðŸš¨ REJECT NULL. The volatility in Rio is STRUCTURAL.\n",
      "Standard Buffers will fail because the variance is statistically distinct.\n",
      "\n",
      "\n",
      "=== HYPOTHESIS B: THE PREMIUM PARADOX (Chi-Square) ===\n",
      "Contingency Table (Count of Orders):\n",
      "is_late   False  True \n",
      "pay_tier              \n",
      "High Pay   1975    382\n",
      "Low Pay    2247    346\n",
      "\n",
      "High Pay Late Rate: 16.2%\n",
      "Low Pay Late Rate:  13.3%\n",
      "\n",
      "Chi-Square Statistic: 7.8436\n",
      "P-Value: 0.0051\n",
      "\n",
      "[VERDICT]\n",
      "REJECT NULL. The system works. Paying more reduces lateness.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# --- 1. LOAD THE EVIDENCE ---\n",
    "csv_path = 'master_logistics_table.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    print(\"âš¡ Loading Master Table from Cache...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    print(\"âš ï¸ CSV Cache missing. Please run the 'Great Unification' script first.\")\n",
    "    # Fallback to empty df if missing to prevent crash in explanation\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. PREPROCESSING ---\n",
    "    cols = ['order_delivered_customer_date', 'order_delivered_carrier_date', 'order_estimated_delivery_date']\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    # Feature Engineering\n",
    "    df['route'] = df['seller_state'] + ' -> ' + df['customer_state']\n",
    "    df['transit_days'] = (df['order_delivered_customer_date'] - df['order_delivered_carrier_date']).dt.total_seconds() / 86400\n",
    "    df['lateness_days'] = (df['order_delivered_customer_date'] - df['order_estimated_delivery_date']).dt.total_seconds() / 86400\n",
    "    df['is_late'] = df['lateness_days'] > 0\n",
    "\n",
    "    # --- HYPOTHESIS A: THE \"RIO CHAOS\" THEORY (Variance Test) ---\n",
    "    print(\"\\n=== HYPOTHESIS A: RIO CHAOS (Variance Test) ===\")\n",
    "    \n",
    "    # 1. Isolate the populations\n",
    "    rio_transit = df[df['route'] == 'SP -> RJ']['transit_days'].dropna()\n",
    "    rs_transit = df[df['route'] == 'SP -> RS']['transit_days'].dropna()\n",
    "\n",
    "    print(f\"Sample Sizes: Rio (n={len(rio_transit)}), RS (n={len(rs_transit)})\")\n",
    "    print(f\"Rio Std Dev: {rio_transit.std():.2f} days\")\n",
    "    print(f\"RS Std Dev:  {rs_transit.std():.2f} days\")\n",
    "\n",
    "    # 2. Levene's Test (Robust to non-normal distributions)\n",
    "    # H0: Variances are Equal\n",
    "    # H1: Variances are Different\n",
    "    stat, p_val_var = stats.levene(rio_transit, rs_transit)\n",
    "\n",
    "    print(f\"Levene Statistic: {stat:.4f}\")\n",
    "    print(f\"P-Value: {p_val_var:.20f}\")\n",
    "\n",
    "    print(\"\\n[VERDICT]\")\n",
    "    if p_val_var < 0.05:\n",
    "        print(\"ðŸš¨ REJECT NULL. The volatility in Rio is STRUCTURAL.\")\n",
    "        print(\"Standard Buffers will fail because the variance is statistically distinct.\")\n",
    "    else:\n",
    "        print(\"FAIL TO REJECT. The variance is normal. We can fix this with math.\")\n",
    "\n",
    "\n",
    "    # --- HYPOTHESIS B: THE \"PREMIUM PARADOX\" (Independence Test) ---\n",
    "    print(\"\\n\\n=== HYPOTHESIS B: THE PREMIUM PARADOX (Chi-Square) ===\")\n",
    "    \n",
    "    # Filter for the Crime Scene (SP -> RJ)\n",
    "    df_rio = df[df['route'] == 'SP -> RJ'].copy()\n",
    "\n",
    "    # 1. Create Freight Tiers (High vs Low)\n",
    "    high_thresh = df_rio['freight_value'].quantile(0.75)\n",
    "    low_thresh = df_rio['freight_value'].quantile(0.25)\n",
    "    \n",
    "    # We compare Top 25% payers vs Bottom 25% payers\n",
    "    df_rio['pay_tier'] = np.where(df_rio['freight_value'] >= high_thresh, 'High Pay', \n",
    "                                  np.where(df_rio['freight_value'] <= low_thresh, 'Low Pay', 'Mid'))\n",
    "    \n",
    "    # Filter out the middle to sharpen the contrast\n",
    "    df_test = df_rio[df_rio['pay_tier'] != 'Mid']\n",
    "\n",
    "    # 2. Contingency Table\n",
    "    # Rows: Payment Tier, Cols: Late Status\n",
    "    contingency = pd.crosstab(df_test['pay_tier'], df_test['is_late'])\n",
    "    \n",
    "    print(\"Contingency Table (Count of Orders):\")\n",
    "    print(contingency)\n",
    "    \n",
    "    # Calculate Failure Rates for context\n",
    "    high_fail = contingency.loc['High Pay', True] / contingency.loc['High Pay'].sum() * 100\n",
    "    low_fail = contingency.loc['Low Pay', True] / contingency.loc['Low Pay'].sum() * 100\n",
    "    print(f\"\\nHigh Pay Late Rate: {high_fail:.1f}%\")\n",
    "    print(f\"Low Pay Late Rate:  {low_fail:.1f}%\")\n",
    "\n",
    "    # 3. Chi-Square Test of Independence\n",
    "    # H0: Late Status is Independent of Payment (Money doesn't help)\n",
    "    # H1: Late Status depends on Payment (Money buys speed)\n",
    "    chi2, p_val_chi, dof, expected = stats.chi2_contingency(contingency)\n",
    "\n",
    "    print(f\"\\nChi-Square Statistic: {chi2:.4f}\")\n",
    "    print(f\"P-Value: {p_val_chi:.4f}\")\n",
    "\n",
    "    print(\"\\n[VERDICT]\")\n",
    "    if p_val_chi > 0.05:\n",
    "        print(\"ðŸš¨ FAIL TO REJECT NULL. The system is 'Fraudulent'.\")\n",
    "        print(\"Paying more does NOT statistically improve reliability. The bottleneck ignores price.\")\n",
    "    else:\n",
    "        print(\"REJECT NULL. The system works. Paying more reduces lateness.\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Could not load data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8c3a14-8f9e-4c98-bb01-78edf336a1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Executing 'Project Rio Bypass' Query...\n",
      "âœ… Generated Pilot List: 500 SKUs identified.\n",
      "\n",
      "[Preview: The Top 10 'Must-Stock' Items for Rio]\n",
      "                      product_id  product_category_name  total_orders_in_rio  total_revenue_in_rio  avg_item_price  avg_current_freight_cost\n",
      "d1c427060a0f73f6b889a5c7c61f2ac4 informatica_acessorios                  102              14724.53          144.36                     42.51\n",
      "53b36df67ebb7c41585e8d54d6772e08     relogios_presentes                   75               8617.79          114.90                      6.22\n",
      "422879e10f46682990de24d770e7f83d     ferramentas_jardim                   72               3993.10           55.46                     16.19\n",
      "53759a2ecddad2bb87a079a1f1519f73     ferramentas_jardim                   69               3783.60           54.83                     19.35\n",
      "99a4788cb24856965c36a24e339b6058        cama_mesa_banho                   69               6104.50           88.47                     17.55\n",
      "368c6c730842d78016ad823897a372db     ferramentas_jardim                   55               2956.60           53.76                     18.26\n",
      "389d119b48cf3043d311335e499d9c6b     ferramentas_jardim                   49               2709.90           55.30                     18.15\n",
      "aca2eb7d00ea1a7b8ebd4e68314663af       moveis_decoracao                   49               3481.20           71.04                     12.53\n",
      "c4baedd846ed09b85f78a781b522f126     ferramentas_jardim                   44               4412.71          100.29                     43.55\n",
      "2b4609f8948be18874494203496bc318           beleza_saude                   39               3429.61           87.94                     15.10\n",
      "\n",
      "[Strategic Insight]\n",
      "Out of the Top 500 Vital Items, 59 are 'Bed/Bath/Table'.\n",
      "This confirms that the 'Bulky Item' strategy is the correct lever.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# --- 1. CONNECT TO THE WAREHOUSE ---\n",
    "db_path = '../03_Database/olist.db'\n",
    "\n",
    "# Robust connection logic\n",
    "if not os.path.exists(db_path):\n",
    "    if os.path.exists('olist.db'):\n",
    "        db_path = 'olist.db'\n",
    "    else:\n",
    "        print(\"âš ï¸ Database not found. Please ensure the 'Great Unification' step was run.\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# --- 2. EXECUTE DELEGATION BRIEF NO. 6 ---\n",
    "# Objective: Identify Top 500 SKUs for \"Project Rio Bypass\"\n",
    "# Logic: Filter for RJ Destination -> Rank by Order Volume\n",
    "\n",
    "query_vital_500 = \"\"\"\n",
    "SELECT \n",
    "    p.product_id,\n",
    "    p.product_category_name,\n",
    "    COUNT(oi.order_id) AS total_orders_in_rio,\n",
    "    -- Impact: Revenue at Risk\n",
    "    ROUND(SUM(oi.price), 2) AS total_revenue_in_rio,\n",
    "    -- Context: Unit Economics\n",
    "    ROUND(AVG(oi.price), 2) AS avg_item_price,\n",
    "    -- Context: Logistics Cost (Current Pain)\n",
    "    ROUND(AVG(oi.freight_value), 2) AS avg_current_freight_cost\n",
    "FROM \n",
    "    order_items oi\n",
    "JOIN \n",
    "    orders o ON oi.order_id = o.order_id\n",
    "JOIN \n",
    "    products p ON oi.product_id = p.product_id\n",
    "JOIN \n",
    "    customers c ON o.customer_id = c.customer_id\n",
    "WHERE \n",
    "    -- FILTER 1: The Problem Zone (Rio)\n",
    "    c.customer_state = 'RJ'\n",
    "    -- FILTER 2: Active Timeline (Post-2017 to capture relevant trends)\n",
    "    AND o.order_purchase_timestamp >= '2017-01-01'\n",
    "GROUP BY \n",
    "    p.product_id, \n",
    "    p.product_category_name\n",
    "ORDER BY \n",
    "    -- PRIORITY: Volume (Stopping the highest number of complaints)\n",
    "    total_orders_in_rio DESC\n",
    "LIMIT 500;\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸš€ Executing 'Project Rio Bypass' Query...\")\n",
    "df_pilot = pd.read_sql_query(query_vital_500, conn)\n",
    "conn.close()\n",
    "\n",
    "# --- 3. THE HANDOFF ARTIFACT ---\n",
    "print(f\"âœ… Generated Pilot List: {len(df_pilot)} SKUs identified.\")\n",
    "print(\"\\n[Preview: The Top 10 'Must-Stock' Items for Rio]\")\n",
    "# We display the top 10 rows so you can put this screenshot in your slide deck\n",
    "print(df_pilot.head(10).to_string(index=False))\n",
    "\n",
    "# Optional: Check the \"Bed/Bath\" dominance in the pilot list\n",
    "bb_count = df_pilot[df_pilot['product_category_name'] == 'cama_mesa_banho'].shape[0]\n",
    "print(f\"\\n[Strategic Insight]\")\n",
    "print(f\"Out of the Top 500 Vital Items, {bb_count} are 'Bed/Bath/Table'.\")\n",
    "print(f\"This confirms that the 'Bulky Item' strategy is the correct lever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cd3f4-56e1-4a0e-b00e-6ed9116110ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
